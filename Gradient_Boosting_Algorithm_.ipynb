{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Introduction**"
      ],
      "metadata": {
        "id": "rPn2n-ymAQRb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient boosting is a method standing out for its prediction speed and accuracy, particularly with large and complex datasets. \n",
        "\n",
        "\n",
        "We already know that errors play a major role in any machine learning algorithm. There are mainly two types of error, bias error and variance error. \n",
        "\n",
        "\n",
        "**Bias is the difference between our actual and predicted values**. Bias is the simple assumptions that our model makes about our data to be able to predict new data.\n",
        "\n",
        "When the Bias is high, assumptions made by our model are too basic, the model can’t capture the important features of our data. This means that our model hasn’t captured patterns in the training data and hence cannot perform well on the testing data too. If this is the case, our model cannot perform on new data and cannot be sent into production. \n",
        "\n",
        "This instance, where the model cannot find patterns in our training set and hence fails for both seen and unseen data, is called Underfitting. \n",
        "\n",
        "***Gradient boost algorithm helps us minimize bias error of the model.***\n"
      ],
      "metadata": {
        "id": "vDOFZSeSAXM5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is Variance?**\n",
        "\n",
        "\n",
        "Variance is the very opposite of Bias. During training, it allows our model to ‘see’ the data a certain number of times to find patterns in it. If it does not work on the data for long enough, it will not find patterns and bias occurs. On the other hand, if our model is allowed to view the data too many times, it will learn very well for only that data. It will capture most patterns in the data,  but it will also learn from the unnecessary data present, or from the noise.\n",
        "\n",
        "We can define variance as the model’s sensitivity to fluctuations in the data. Our model may learn from noise. This will cause our model to consider trivial features as important. \n",
        "\n"
      ],
      "metadata": {
        "id": "KZmjyK1IFej2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Overfitting** : Hence, our model will perform really well on testing data and get high accuracy but will fail to perform on new, unseen data. New data may not have the exact same features and the model won’t be able to predict it very well. This is called Overfitting. \n",
        "\n",
        "**Underfitting** : where the model cannot find patterns in our training set and hence fails for both seen and unseen data, is called Underfitting.\n"
      ],
      "metadata": {
        "id": "dqaDWuLgF3x6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comparison Table of Adaboost and Gradient Boosting**\n",
        "\n",
        "\n",
        "**1. Features\tGradient boosting\tAdaboost**\n",
        "\n",
        "Model\tIt identifies complex observations by huge residuals calculated in prior iterations\tThe shift is made by up-weighting the observations that are miscalculated prior\n",
        "Trees\tThe trees with week learners are constructed using a greedy algorithm based on split points and purity scores. The trees are grown deeper with eight to thirty-two terminal nodes. The week learners should stay a week in terms of nodes, layers, leaf nodes, and splits\tThe trees are called decision stumps.\n",
        "\n",
        "**2. Classifier**\tThe classifiers are weighted precisely and their prediction capacity is constrained to learning rate and increasing accuracy\tEvery classifier has different weight assumptions to its final prediction that depend on the performance.\n",
        "**3. Prediction**\tIt develops a tree with help of previous classifier residuals by capturing variances in data.\n",
        "The final prediction depends on the maximum vote of the week learners and is weighted by its accuracy.It gives values to classifiers by observing determined variance with data. Here all the week learners possess equal weight and it is usually fixed as the rate for learning which is too minimum in magnitude.\n",
        "Short-comings\tHere, the gradients themselves identify the shortcomings.\tMaximum weighted data points are used to identify the shortcomings.\n",
        "\n",
        "**Loss value**\t \n",
        "Gradient boosting cut down the error components to provide clear explanations and its concepts are easier to adapt and understand\n",
        "\n",
        "\n",
        "The exponential loss provides maximum weights for the samples which are fitted in worse conditions.\n",
        " \n",
        "\n",
        "Applications\t\n",
        "\n",
        "This method trains the learners and depends on reducing the loss functions of that week learner by training the residues of the model\tIts focus on training the prior miscalculated observations and it alters the distribution of the dataset to enhance the weight on sample values which are hard  for classification\n",
        "\n",
        "***Conclusion***\n",
        "\n",
        "So, when it comes to Adaptive boosting the approach is done by up-lifting the weighted observation which is misclassified prior and used to train the model to give more efficacy. In gradient boosting, the complex observations are computed by large residues left on the previous iteration to increase the performance of the existing model.\n",
        "\n"
      ],
      "metadata": {
        "id": "Vv6rYy2zGT-N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lZva-4g4IVzz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZAGVGeY6rsy"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas\n",
        "BIKE = pandas.read_csv(\"day.csv\")\n",
        " \n",
        "#Separating the depenedent and independent data variables into two dataframes.\n",
        "from sklearn.model_selection import train_test_split \n",
        "X = bike.drop(['cnt'],axis=1) \n",
        "Y = bike['cnt']\n",
        "# Splitting the dataset into 80% training data and 20% testing data.\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.20, random_state=0)\n",
        " \n",
        "import numpy as np\n",
        "def MAPE(Y_actual,Y_Predicted):\n",
        "    mape = np.mean(np.abs((Y_actual - Y_Predicted)/Y_actual))*100\n",
        "    return mape\n",
        " \n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "GR = GradientBoostingRegressor(n_estimators = 200, max_depth = 1, random_state = 1) \n",
        "gmodel = GR.fit(X_train, Y_train) \n",
        "g_predict = gmodel.predict(X_test)\n",
        "GB_MAPE = MAPE(Y_test,g_predict)\n",
        "Accuracy = 100 - GB_MAPE\n",
        "print(\"MAPE: \",GB_MAPE)\n",
        "print('Accuracy of Linear Regression: {:0.2f}%.'.format(Accuracy))"
      ]
    }
  ]
}